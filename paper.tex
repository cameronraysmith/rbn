\input{header.tex}

\begin{document}

% Add Figure, Table prefixes to references
% http://tex.stackexchange.com/a/6063/6784
\let\ref\autoref

\title{Ensemble Stability of Biological Networks}

\pagenumbering{arabic}

% Title must be 150 characters or less
\begin{center}
{\Large
\textbf{Ensemble Stability of Biological Networks}
}
% Insert Author names, affiliations and corresponding author email.
\\[.5cm]
Aviv Bergman$^{1,2,3,4}$,
Raymond S. Puzio$^{1}$,
Cameron Smith$^{1}$,
\\[.5cm]
$^1$Department of Systems and Computational Biology,\\
$^2$Dominick P. Purpura Department of Neuroscience,\\
$^3$Department of Pathology, Albert Einstein College of Medicine,\\
1301 Morris Park Ave, Bronx, NY 10461, USA\\
$^4$Santa Fe Institute, 1399 Hyde Park Road, Santa Fe, NM 87501, USA
\\[.5cm]
% $\ast$To whom correspondence should be addressed; E-mail: aviv@einstein.yu.edu.
\end{center}

{\begin{quote} \bf
\input{tex/abstract}
\end{quote}}

% \section{Todo}
% \listoftodos

\section{Introduction}

The traditional approach taken in the study of chemical reaction, gene-regulatory, population, and ecosystem networks is to consider a particular example, derive a system of differential equations to model that example, try to fit the model to data and adjust the modeling assumptions along with parameter values until a good fit is achieved. Recent work has demonstrated that as a result of sloppiness in the dependence of qualitative dynamic phenomena on the geometry of parameter space that this approach allows for a large variety of models to ``fit the data'' \cite{Brown2003,Gutenkunst2007,Daniels2008a,Machta2013,Hines2014,Prabakaran2014}. In the face of uncertainty about the structure of such biological networks, to model the components under consideration as randomly interlinked becomes a reasonable approximation. This approach enables one to gain insight into what dynamical phenomena are possible to observe within a given class of dynamical systems, which is necessary to understand in order to determine whether or not a particular dynamical phenomenon should be regarded as unique or generic in the development and investigation of models applied to particular systems \cite{Gunawardena2013,Gunawardena2014}.

Indeed, investigating generic properties of a large class of dynamical systems was the approach taken by May in models of ecosystem dynamics \cite{Gardner1970,May1972}. The class of dynamical systems studied by May is so general that to restrict its applicability to ecosystem dynamics is certainly not necessary and perhaps even inefficient with respect to the goal of understanding how biological networks operate across the various relevant levels of organization. For example, the study of chemical reaction, population, and gene-regulatory networks all utilize essentially equivalent mathematical structure as that discussed by May in the context of ecosystem dynamics \cite{RossCr2003,Alon2006,Palsson2006,HamidBolouri2008,Palsson2011a,Voit2012,Sauro2012}. Developing unified mathematical descriptions of all of these is one of the paramount goals of systems biology. This incredibly generic applicability of gaining a better understanding of the class of models investigated by May demonstrates the unequivocal value of deeper investigation.  However, work attempting to continue the development of the so-called May-Wigner stability theorem revealed that May's conjectured stability criterion was not as easy to demonstrate as was initially believed \cite{Cohen1984,May1972a,Radius2014}.

Here we build on work aimed at investigating the stability of randomly connected dynamical systems to state-based perturbations. We precisely compute the probability of stability as a distribution over system connectivity for all systems containing two interacting components. We then proceed to determine the probability of structural stability over the same domain. We find that while stability to perturbations in the state of this class of dynamical systems is independent of system connectivity, perturbations to the magnitude of connections is correlated with connectivity in a non-linear manner. This work reestablishes and clarifies a thread of research that is extremely important for the continuing development of systems biology.

\section{Stability analysis of biological networks}\label{sec:stabanalbn}
In the construction of a class of potential models for biological systems at any level of the biological hierarchy from metabolic to ecosystem-level networks, it is common to first attempt to define a collection of observable phenomena of interest and determine a domain (such as binary numbers, integers, or real numbers) in which each observable can be quantified. Next, it is necessary to establish the interdependencies among system components. The specific manner in which the components depend upon one another must be clarified, which is often done by determining a particular system of mathematical functions that represents a hypothesis about how the quantified observables evolve in time. To the degree to which there is uncertainty about the interactions among system components, parameters are introduced to broaden the class of models under consideration. Finally, whatever model class remains may be compared to empirical observations to determine how capable the model is of representing the phenomena of interest.

For the case of continuous deterministic observables, the above process can be made more precise by associating a manifold $M_i$ to each observable, a directed graph $G$ to the interdependencies, and a vector field $V \in T(\prod_i M_i)$ over the space determined by taking the product of the manifolds associated to the collection of observables, satisfying these interdependencies for each observable  \cite{Deville}. For example, if we have two observables $\{x_1,x_2\}$ where the domains in which they are quantified are given by manifolds $\{M_1,M_2\}$ such that $x_1 \in M_1 \equiv \mathbb{R}^1$ and $x_2 \in M_2 \equiv \mathbb{R}^1$, a directed graph $G_X$ describing the dependencies between these observables and vector field $V$ with components $\{F_1,F_2\}$ defined on $M_1 \otimes M_2 \equiv \mathbb{R}^2$ satisfying the dependencies determined by $G_X$. If the system under consideration has the graph given in \ref{fig:examplesystemmodules}A
% \begin{center}
% \input{tex/examplehamdyngraph}
% \end{center}
with adjacency matrix
$$
\adj(G_X) = \begin{pmatrix}
0 & 1 \\
1 & 1
\end{pmatrix}
$$
then for the system $X = \{G_X, M_X, F_X\}$, where $M_X \equiv \{M_1,M_2\}$ and $F_X \equiv \{F_1,F_2\}$ such that $F_1$ is constrained to be a function of $x_2$ and $F_2$ is a function of both $x_1$ and $x_2$ yielding the flow equations
\begin{align*}
\frac{dx_1}{dt} & = F_1(x_2)\\
\frac{dx_2}{dt} & = F_2(x_1,x_2)
\end{align*}

In the more general case of a system with $n$ components we would have an $n$-dimensional vector of observables
$$
x(t) = (x_1(t), \ldots x_n(t)) = \vec{x}(t)
$$
whose components are solutions to the arbitrary first order system
$$
\frac{dx_i(t)}{dt} = F_i(\vec{x}(t)), \; (i=1,\ldots,n)
$$
where $F_i$ represent, potentially nonlinear, functions of the given vector of state variables.

In order to accommodate the possibility of uncertainty in our modelling,
we will generalize our notion of a system on a network to that of a
system with random parameters.  Again, this will involve three
steps. First, we provide a measure space $S, \mathcal{B}(S)$ which represents the
values over which our parameters can vary. Next, instead of a single
vector field, we consider a family of vector fields parameterized by
this space. That is to say, we introduce a mapping $V \colon S \to \Gamma T(\prod_i M_m)$.
(Here $\Gamma$ denoted the space of sections of the tangent bundle.)
As before, for each point $p \in S$, the vector field $V(p)$ must be consistent with
the system graph. Finally, we select a probability measure $\mu \colon
\mathcal{B}(S) \to [0,1]$ which represents our understanding of which values of the parameters
are likely. In accord with Bayesian statistics, we might revise this
distribution as data comes in or use it to estimate parameters and error
bars from experimental results.

Having done this, we are now in a position to do a probabilistic analysis.
Given some quantity $q$ characterizing our system, this quantity now
becomes a random variable.  This variable may be discrete of continuous
depending on the quantity under consideration.  For example, if the quantity
is the time it takes for a particle to travel between two points, we have
a real-valued random variable; if the quantity is the number of fixed
points, we have an integer-valued random variable; and if the quantity is
whether or not the system posesses a limit cycle, we have a binary random
variable.

Another question one can ask is that of robustness, which measures the
probability that, if some property holds for a set of parameters, it will
continue to hold if we make a random perturbation about those parameter values.
To define robustness, we will introduce, in addition to the probability
distribution $\mu$ described above, a family of probability distributions
$\mu' \colon S \times \mathcal{B}(S) \to [0,1]$.  Then, given a binary random
variable $q$, we we define its robustness as the following conditional proability:
\[
  \mathrm{Robust} (q) =
  \frac{\int_S d\mu(x) \int_S d\mu'(y) \mathbf{1}_q(x) \mathbf{1}_q(y)}
  {\int_S d\mu(x) \mathbf{1}_q(x)}
\]

This description can be formalized as a directed graph $G_X$ that describes the manner in which each of the variables depends upon one another given by an adjacency matrix $\adj(G_X)$ where
 \begin{displaymath}
   \adj(G_X)_{ij} = \left\{
     \begin{array}{ll}
       1, & F_i \hbox{ depends on } x_j\\
       0, & F_i \hbox{ does not depend on } x_j
     \end{array}
   \right.
\end{displaymath}
A graph can be decomposed into a natural collection of \emph{modules} called strongly connected components. A strongly connected component of a graph is a maximal subset of vertices where each vertex within the subset can be reached from any other \cite{Cormen2009}. The strongly connected components of some examples of three component systems are outlined in \ref{fig:scc}.

Linearization of the $F_i$ about any equilibrium solution $\vec{x}^0$ such that $F_i(\vec{x}^0)=0$ leads to the system
\begin{equation}\label{eq:lineardynsys}
\frac{d\vec{y}(t)}{dt} = A \vec{y}(t),
\end{equation}
where $\vec{y} = \vec{x} - \vec{x}^0$ and the $n \times n$-matrix $A$ has components
$$
a_{ij} = \left. \frac{\partial F_i}{\partial x_j} \right|_{\vec{x} = \vec{x}^0}.
$$
To each dynamical system having Jacobian matrix $A$ at some fixed point $\vec{x}^0$ we can associate a directed graph $G_A$ given by an adjacency matrix $\adj(G_A)$ where
 \begin{displaymath}
   \adj(G_A)_{ij} = \left\{
     \begin{array}{lr}
       1, & a_{ij} \neq 0\\
       0, & a_{ij} = 0
     \end{array}
   \right.
\end{displaymath}
The graphs $G_X$ and $G_A$ are equivalent because the condition $F_i$ independent of $x_j$ definitive of $\adj(G_X)$ corresponds precisely to $\frac{\partial F_i}{\partial x_j}=0$.

The spectral abscissa of the matrix $A$ is defined as
$$
\eta(A) = \max_i \{\Re(\lambda_i)\}
$$
where $\lambda_i$ are the eigenvalues of $A$. The system defined by $F_i$ and $\vec{x}^0$ is dynamically stable if the spectral abscissa of $A$ is less than zero, equivalently, $\eta(A) < 0$. This is because the general solution to \ref{eq:lineardynsys} is
$$
y_i(t) = \sum_j b_{ij} e^{\lambda_j t}, \; (i=1,\ldots,n)
$$
for some matrix $B=(b_{ij})$ and thus all $\vec{y} = \vec{x} - \vec{x}^0$ decay to zero when all $\lambda_i < 0$.

To determine whether a randomly selected dynamical system evaluated at a random critical point, given by a matrix such as $A$, is stable, it is sufficient to check the above condition. Integrating over the region of the parameter space defining $A$ enables the determination of the probability of stability to perturbations in state $\vec{x}$ over an ensemble of dynamical systems.

Another form of stability, referred to as structural stability \cite{Smale1967}, requires the determination of whether or not a given dynamical system that is determined to be stable remains stable under a perturbation to one of its defining parameters. This can be formalized as the conditional probability distribution
$$
P(A' \, \textrm{stable}\, \big| \, A \, \textrm{stable})
$$
where $A'$ represents a matrix derived from some perturbation applied to the matrix $A$ given above. In the simple case where $A \in \mathbb{R}^{n \times n}$ the perturbation which involves sampling uniformly over the $a_{ij}$ defining $A$ the value of a particular $a_{ij}$ from a given probability distribution.

\section{Stability criteria}
Suppose that $M$ is an $n \times n$ matrix with real coefficients.
Motivated by the discussion of \ref{sec:stabanalbn}, we will call
the matrix $M$ stable if all its eigenvalues have negative real part.
Define the characteristic polynomial as $\chi(M)(z) = \det(zI - M)$.
Denote the coefficients of $\chi(M)$ as $s_j$ and its roots as $r_j$,
writing $p(x) = x^n + \sum_{k=1}^n s_k x^{n-k} = \prod_{k=1}^n (x-r_k)$.
Then, since the roots of $\chi$ are the eigenvalues of $M$, asking that
$M$ be stable is equivalent to asking that the roots of $\chi$ all have
negative real part.  We will formulate conditions for this to happen in
terms of the coefficients of $\chi$.

\begin{lemma}
If $M$ is stable, then $s_i > 0$ for all $i$ between $1$ and $n$.
\end{lemma}

\begin{proof}
Assume that $M$ is stable and that it has $2m$ complex roots which come in
complex conjugate pairs and $n-m$ real roots.  Let $r_1 \ldots, r_{2m}$ be
the complex roots with $r_{2k+1}$ being the complex conjugate of $r_{2k}$
and let $r_{2m+1}, \ldots,  r_{n}$ be the real roots. Then we can write the
factorization as follows:
\[
\chi(M)(z) = \prod_{k=1}^{m} (z^2 - (r_{2k} + r_{2k+1}) z + r_{2k} r_{2k+1})
             \prod_{k=2m+1}^{n} (z - r_k)
\]
Since $r_{2k}$ and $r_{2k+1}$ are complex conjugates, $r_{2k} + r_{2k+1} =
\Re r_{2k} = \Re r_{2k+1}$ and $r_{k} r_{2k+1} > 0$.   Since all the
roots have negative real part, all the coefficients of the terms in
the products above are positive so, when we multiply out the product,
we will have all positive terms.  Hence, we conclude that a necessary
condition for $M$ to be stable is that $s_k > 0$ for all $k$.
\end{proof}

To state the next criterion, we will require the quantity $\Delta$, which
is defined as $\prod_{j=1}^{n-1} \prod_{k=j+1}^n (r_1 + r_j)$.  Since
this expression is invariant under permuting the roots, it may be expressed
as a function of the coefficients $s_i$; for small values of $n$, the
resulting expression looks as follows:
\begin{align*}
n=2 \qquad &\Delta = s_1 \\
n=3 \qquad &\Delta = s_3 - s_1 s_2 \\
n=4 \qquad &\Delta = s_1 s_2 s_3 - s_3^2 - s_1^2 s_4 \\
n=5 \qquad &\Delta = s_1 s_2 s_3 s_4 - s_3^2 s_4 - s_1^2 s_4^2 -
                     s_1 s_2^2 s_5 + s_2 s_3 s_5 + 2 s_1 s_4 s_5 - s_5^2
\end{align*}

\begin{lemma}
Given a polynomial $p(x) = z^n + \sum_{k=1}^{n-1} s_k x^{n-k}$ and a real number
$\epsilon > 0$, there exists $\delta > 0 \in \mathbb{R}$ such that, if $q(x) =
x^n + \sum_{k=1}^{n-1} \sigma_i x^i$ is a polynomial for which $|s_i - \sigma_i|
< \delta$ then one can order the roots of $p$ and $q$ such that $p(z) = \prod_{k=1}^n
(z - r_i)$ and $q(z) = \prod_{k=1}^n (z - \rho_i)$ and $| r_k - \rho_k | < \epsilon$.
\end{lemma}

\begin{proof}
Without loss of generality, we may assume that $\epsilon < 1$.

We first consider the case where $p(x) = x^n$.

If we make the rescaling $x = \epsilon y/2$ in the equation $p(x)  = 0$,
we obtain the equation
\[
y^n + \sum_{k=1}^{n-1} \left(\frac{2}{\epsilon}\right)^k s_k y^{n-k} = 0 .
\]
Cauchy's bound on the roots of this equation is
\[
|y| < 1 + \max_j \left(\frac{2}{\epsilon}\right)^j |s_j| ,
\]
which becomes the bound
\[
|x| < \frac{\epsilon}{2} + \max_j \left(\frac{2}{\epsilon}\right)^{j-1} |s_j|
< \frac{\epsilon}{2} + \left(\frac{2}{\epsilon}\right)^{n-1} \max_j |s_j|
\]
for the original equation.  Hence, if we choose $\delta = (\epsilon/2)^{n}$,
we will have $|x| < \epsilon$.

Next, we generalize to the case $p = (x-r)^n$.

To extend this result of the general case, we note that, given two
polynomials $p = x^m + \sum_{k=1}^n a_k x^{m-k}$ and
$q = x^n + \sum_{k=1}^n b_k x^{n-k}$, the Jacobian of the map which
sends the coefficients of the matrices to the coefficients of the
product $pq$ is the Sylvester matrix, whose determinant is the resultant
of $p$ and $q$.  Hence, is $p$ and $q$ have no common root, the map
is locally invertible.
\end{proof}

\begin{lemma}
If a point lies on the boundary of the set of stable polynomials, then
either $s_n = 0$ and $\Delta = 0$.
\end{lemma}

\begin{proof}
Suppose that none of the real parts of the roots of a polynomial $p$ equal
zero.  Let $\epsilon$ be the minimum of the absolute values of the real
parts of the roots of $p$.  Then, by continuity of the map which sends the
roots of a polynomial to its coefficients, there exists a $\delta > 0$
such that, if $q$ is any polynomial whose coefficients differ by less than
$\delta$ from the

Suppose that we have a family
of polynomials whose roots are of one type but turn into another type.
There are three ways this could happen:
\begin{itemize}
\item{1.} A real root could go from being positive to being negative
  or vice versa.  For this to happen, we must pass through an
  intermediate value with a zero root.  For that intermediate value,
  we have $D = 0$.
\item{2.} A pair of real roots could turn into a pair of complex roots
  or vice versa.  For this to happen, we must pass through an
  intermediate value with a repeated root.  For that intermediate
  value, we have $\delta = 0$.
\item{3.} A conjugate pair of complex roots could go from having
  positive real part to having negative real part or vice versa.  For
  this to happen, we must pass through an intermediate value in which
  the real part is zero.  For that intermediate value, the two roots
  are imaginary so, bing conjugates, they sum to zero, hence we have
  $\Delta = 0$.
\end{itemize}
\noindent  Thus, we conclude that the boundaries of our regions
$R_{abcd}$ consist of portions of the surfaces $D=0, \delta=0,
\Delta=0$ and we will characterize them by studying these surfaces.

To complete the proof, we show that the condition $\delta = 0$ is not
needed to describe the boundary.  Suppose that $\delta = 0$.  Then there
must exist a repeated root.  If the real part of any root, repeated or
not, is positive, then the polynomial is not stable and, by continuity,
if one allows the coefficients to vary within a sufficiently small
ball about that solution, the root will remain negative, so the
polynomial cannot lie on the boundary of the stable region.  If all
the roots, including the repeated ones, are negative then, by a similar
argument the point is in the interior of the stable region.  The only
other possibility is that some root has zero real part, but then
$\Delta = 0$ so the condition $\delta = 0$ is superfluous.
\end{proof}

Introduce $V$ as the parameter space of all polynomials of degree $n$
with real coefficients.  In other words, $V$ is a real vector space of
dimension $n$ whose coordinates we will identify with the coefficients
$s_k$ of our polynomial.  We now partition this space into regions
corresponding to specific types of polynomials.  Let $R_{abcd}$ be the
subset of $V$ such that $x \in R_{abcd}$ if and only if the polynomial
corresponding to $x$ has $a$ positive real roots, $b$ negative real
roots, $c$ complex roots with positive real part and $d$ complex roots
with negative real part.  Once we characterize these regions using
invariants, we will have solved the original problem because a matrix
is stable if the point of $V$ corresponding to it lies in a region of
the form $R_{a0c0}$.

Next, we introduce some products:
\begin{align}
D &= \prod_{k=1}^n r_n \cr
\delta &= \prod_{j=1}^{n-1} \prod_{k=j+1}^n (r_1 - r_j)^2 \cr
\Delta &= \prod_{j=1}^{n-1} \prod_{k=j+1}^n (r_1 + r_j) \cr
\end{align}

Since these quantities are invariant under permutation of the roots
$r_j$, they may be expressed as functions of the coefficients $s_j$.
The explicit expressions may be obtained by such techniques as long
division and determinants. These quantities are of interest because they describe the boundaries between the regions introduced above.

The simplest case is $n=2$.  There we have the regions $R_{2000},
R_{1100}, R_{0200}, R_{0020}, R_{0002}$ and our invariants are $D =
s_2, \delta = s_1^2 - 4s_2, \Delta = s_1$.  The relation between these
is summarized by the conditions
\begin{align}
R_{2000} \qquad &D > 0, \delta > 0, \Delta > 0, \cr
R_{1100} \qquad &D < 0, \delta > 0, \cr
R_{0200} \qquad &D > 0, \delta > 0, \Delta < 0, \cr
R_{0020} \qquad &\delta < 0, \Delta > 0, \cr
R_{0002} \qquad &\delta < 0, \Delta < 0. \cr
\end{align}
These regions used for the classification of stability of two-component systems are show in \ref{fig:region2x2}. The stable regions are given by $R_{2000}$ and $R_{0020}$. All of the other regions represent unstable matrices.

Moving to $n=3$, our invariants now look as follows: $D = s_3, \delta
= 18 s_1 s_2 s_2 + s_1^2 s_2^2 - 4 s_2^3 - 4 s_1^3 s_3 - 27 s_3,
\Delta = s_1 s_2 - s_3$.  However, now we can no longer simply use their
signs to distinguish regions.  For instance, consider the regions
$R_{3000}$ and $R_{1200}$.  These both have $D > 0$ and $\delta > 0$
but to tell them apart, we need to note that the region $\delta > 0$
has two connected components and use the auxiliary condition that $s_1 > 0$ to
specify which of the components contains our region. The regions analogous to \ref{fig:region2x2} for three component systems where $D=1$ is given in \ref{fig:region3x3}.
%(Looking at the sign of $\Delta$ will not help here because, while $\Delta > 0$ in the region $R_{3000}$, it is also positive for some elements of $R_{1200}$ as well.)

\section{System graph and decomposition into strongly connected components}
Next, we will want to relate prpoperties of our matrix and its eigenvalues
to  properties of the associated graph $G_A$.  To do this, we will first
examine the category of paths on $G_A$, and derive a formula expressing
the chararacteristic polynomial as a sum over cycles.

Given a digraph $G$, let $\Path (a,b)$ denote the set of paths from the
vertex $a$ to the vertex $b$.  Given a path $p_1$ from $a$ to $b$ and a path
$p_2$ from $b$ to $c$, we may concatenate them to obtain a path $p_1 \circ
p_2$ from $a$ to $c$.  Hence, $(\Vertex(G), \Path(G), \circ)$ forms a category.


The characteristic polynomial $\chi(A)$ associated to $A$ can be factored
according to the strongly connected components of the graph $G$.
\[
\frac{\chi(M)'(z)}{\chi(M)(z)} = \tr (zI - M)^{-1}
\]
where, $\chi(M)(z) = \det (zI - M)$ is the chararacteristic polynomial of the
matrix $M$ with $z$ a complex variable.  This formula is readily verified when
$M$ is diagonal and has the nice property of being additive over decompositions
of the matrix. Making the change of variables $x=\frac{1}{z}$, we have
\[
\tr (I - xM)^n = \sum_{n=0}^\infty x^n \sum_{a \in \Vertex(G)} \sum_{p \in \Path_n (G)(a,a)} F(p)
\]
where $G$ is the graph of the matrix and $\Path_n (G;a,b)$ is the set of
paths of length $n$ from $a$ to $b$.  ; $F$ is the functor from this category
to the multiplicative semigroup of the real numbers which corresponds to the matrix $M$.

As an illustration of this formula, we will use it to compute the characteristic
polynomial of the matrix associated to the labelled graph in figure ??.  We begin
by noting that the closed paths of this graph can be described by the following
regular expressions:
\begin{align*}
\Path (1,1) &: (e + bd + bca)^* \\
\Path (2,2) &: (ae^*(be^*d)^*bc)^* \\
\Path (3,3) &: ((ca + d)e^*b)^* \\
\end{align*}
Since these regular expressions are non-redundant, we can obtain generating
functions for the paths by considering $a,b,c,d,e$ as non-commutative
variables and replacing the Kleene star $x^*$ by $(1 - x)^{-1}$:
\begin{align*}
\Path (1,1) &: (1 - e - bd - bca)^{-1} \\
\Path (2,2) &: (1 - a(1-e)^{-1}(1 - b(1-e)^{-1}d)^{-1}bc)^{-1} \\
\Path (3,3) &: (1 - (ca + d)(1 - e)^{-1}b)^{-1} \\
\end{align*}
To obtain the characteristic polynomial, we make the substitutions
$a \mapsto m_{12}/z, b \mapsto m_{31}/z, c \mapsto m_{23}/z, d \mapsto m_{13}/z,
e \mapsto m_{11}/z$ and simplify, using the fact that the matrix entries
commute:
\begin{align*}
\Path (1,1) &: \frac{z^3}
{z^3 - m_{11} z^2 - m_{13} m_{31} z - m_{12} m_{23} m_{31} } \\
\Path (2,2) &: \frac{z(z^2 - m_{11} z - m_{13} m_{31})}
{z^3 - m_{11} z^2 - m_{13} m_{31} z - m_{12} m_{23} m_{31} }  \\
\Path (3,3) &: \frac{z^2(z^2 - m_{11})}
{z^3 - m_{11} z^2 - m_{13} m_{31} z - m_{12} m_{23} m_{31} } \\
\end{align*}
Adding these three quantitites together, we obtain
\[
\frac{z(3 z^2 - 2 m_{11} z - m_{13} m_{31})}
{z^3 - m_{11} z^2 - m_{13} m_{31} z - m_{12} m_{23} m_{31} },
\]
which we recognize as $\chi'(z)/\chi(z)$, where
$\chi(z) = z^3âˆ’m_{11} z^2 - m_{13} m_{31} z - m_{12} m_{23} m_{31}$.
finally, we note that this polynomial is indeed the characteristic
polynomial of the matrix
\[
 \begin{matrix}
   m_{11} & m_{12} & m_{13} \\
   0      & 0      & m_{23} \\
   m_{31} & 0      & 0 \\
 \end{matrix}
\]
corresponding to our digraph.

We now make use of the additivity.  Let $\hier$ be the functor which
sends $G$ to a poset $P$.  Then every element $p$ of $\Path(a,a)$ lies
in $\hier^{-1} (\hier(a))$.  Hence, our sum decomposes into partial sums
for the fibers of the points in the hierarchy which corresponds to a
factorization of the characteristic polynomial.

\section{Stability and robustness analysis of multi-component systems}
For two-component systems having $2 \times 2$ Jacobian matrices, the aforementioned stability criteria result in the conditions $T < 0$ and $D >
0$ where $T$ and $D$ denote the trace and the determinant.

Suppose we have a stable matrix
$$
\begin{pmatrix}
a & b \\
d & c
\end{pmatrix}
$$
where $a + c < 0$ and $ac > bd$. We will consider the case in which each of the parameters defining the matrix is sampled from the uniform distribution $\mathcal{U}(-1,1)$ so that the parameter space corresponds to the $d$-dimensional hypercube, $H^d$, of edge length $r=2$, centered about the origin, and having volume $r^d$.  We want to know the probability that, if we resample an element of this matrix, the result will be stable.  By symmetry, there are two cases to consider; resampling $a$ is equivalent to resampling $c$ and resampling $b$ is equivalent
to resampling $d$.

Suppose that we resample $b$.  We want to compute the probability that a random stable matrix will remain stable upon resampling $b$, which is
% \begin{strip}
\begin{align}\label{eq:condprob}
P\left(\begin{pmatrix}
a & b' \\
d & c
\end{pmatrix} \textrm{stable } \bigg| \begin{pmatrix}
a & b \\
d & c
\end{pmatrix} \textrm{stable } \right)
& = \frac{P\left(\begin{pmatrix}
a & b \\
d & c
\end{pmatrix} \textrm{stable and } \begin{pmatrix}
a & b' \\
d & c
\end{pmatrix} \textrm{stable } \right)}{P\left(\begin{pmatrix}
a & b \\
d & c
\end{pmatrix} \textrm{stable } \right)}.
\end{align}
The denominator of \ref{eq:condprob} is given by
\begin{align*}
P\left(\begin{pmatrix}
a & b \\
d & c
\end{pmatrix} \textrm{stable } \right) = \frac{\int_{\genfrac{}{}{0pt}{}{\genfrac{}{}{0pt}{}{ac>bd}{a+c<0}}{H^4}} da\,db\,dc\,dd\,1}{\int_{H^4} da\,db\,dc\,dd\,1}.
\end{align*}
Since the trace does not involve $b$, the $T<0$ condition will be satisfied automatically and we only need to examine the determinant. Thus, we have the inequalities $ac > b'd$ and $-1 < b' < 1$ in addition to the previous constraints leading to an expression for the numerator of \ref{eq:condprob}
\begin{align*}
P\left(\begin{pmatrix}
a & b \\
d & c
\end{pmatrix} \textrm{stable and } \begin{pmatrix}
a & b' \\
d & c
\end{pmatrix} \textrm{stable } \right) = \frac{\int_{{{ac>b'd \atop ac>bd} \atop a+c<0} \atop H^5} da\,db\,dc\,dd\,db'\,1}{\int_{H^5} da\,db\,dc\,dd\,db'\,1}.
\end{align*}
The analogous equation for resampling $a$ is
\begin{align*}
P\left(\begin{pmatrix}
a & b \\
d & c
\end{pmatrix} \textrm{stable and } \begin{pmatrix}
a' & b \\
d & c
\end{pmatrix} \textrm{stable } \right) = \frac{\int_{{{{a'c>bd \atop a' + c < 0} \atop ac>bd} \atop a+c<0} \atop H^5} da\,db\,dc\,dd\,da'\,1}{\int_{H^5} da\,db\,dc\,dd\,da'\,1}.
\end{align*}

The probability of \emph{a priori} stability and of stability to perturbations for all two component systems is given in Table \ref{tab:structstabmat}. Similarly, the analogous results for all three component systems is given in Table \ref{tab:structstabmat3}.

\section{Connectivity is correlated with robustness}
\ref{fig:stab3x3} shows the probability of stability to perturbations for all three component systems as a function of connectivity.

\section{Cycle number is inversely correlated with robustness}
\ref{fig:cycle3x3} shows the probability of stability to perturbations for all three component systems as a function of the number of simple cycles (elementary circuits) of length greater than one in the corresponding directed graph \cite{Johnson1975}.

\section{Cycle number and connectivity classify robust systems}
\ref{fig:connectcycle3D3x3} shows the probability of stability to perturbations for all three component systems as a function of connectivity and number of cycles.

\section{Hierarchical systems are the most robust}
\ref{fig:scc} shows systems with varying levels of hierarchy and modularity.

\section{Discussion}

\newpage

\bibliographystyle{amsplain}
\bibliography{bib/books,bib/papers}

\newpage
\FloatBarrier

\section{Figure Legends}
\input{tex/figures}

\newpage
\FloatBarrier

\section{Tables}
\input{tex/table2x2stab}
\input{tex/tablenographs}
% \input{tex/tablewithgraphs}

\end{document}

%------------------------------------------------------------------------------
% End of journal.tex
%------------------------------------------------------------------------------
